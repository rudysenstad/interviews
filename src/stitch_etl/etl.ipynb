{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from db import DatabaseHandler\n",
    "from sqlalchemy import create_engine, Column, Integer, String,text\n",
    "import pandas as pd\n",
    "import openai as oa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate db handler and create tables if they don't exist\n",
    "dbh = DatabaseHandler()\n",
    "dbh.create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "# we know the size of the file and that it can be loaded into memory\n",
    "# in the case of a larger file, we can ingest the first X rows depending on design\n",
    "# in a queue based load pattern, it is defined by configurations \n",
    "# whatever the case, the ingestion needs to be tuned to the input data type and volume\n",
    "raw_df = pd.read_csv(\"./dataset/rotten_tomatoes_movies.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can receive it through message queue/pubsup(kafka) or file uploads (S3,buckets,etc)\n",
    "# in this example we have a single file of unknown size\n",
    "# parse csv file\n",
    "credit_types = set()\n",
    "genres = set()\n",
    "person = set()\n",
    "\n",
    "# person\n",
    "\n",
    "# credit type\n",
    "\n",
    "# genre\n",
    "\n",
    "# movie\n",
    "\n",
    "# credit (person,credit,movie abstract)\n",
    "\n",
    "# movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the categorical columns that are one-to-many (credit type, genre)\n",
    "# these require deduplicating and finding the unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "client = OpenAI()\n",
    "loader = TextLoader(\"grims.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index_name = \"testindex\"\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "index_name = \"groqtest\" # Use the exact index name you used before \n",
    "embeddings = OpenAIEmbeddings() \n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "query = \"What was the scissor-grinder singing?\" \n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
